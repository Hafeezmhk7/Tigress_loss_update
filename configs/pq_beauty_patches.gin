# =============================================================================
# PQ-VAE Training Configuration (Patch-Based Product Quantization)
# =============================================================================
# This config trains PQ-VAE with patch-based semantic encoding
# instead of global embeddings, using 4 independent codebooks.

# Training Parameters
train.iterations = 50000
train.batch_size = 128
train.learning_rate = 0.0001
train.weight_decay = 0.01
train.gradient_accumulate_every = 1
train.save_model_every = 10000
train.eval_every = 5000
train.log_every = 1000

# Dataset
train.dataset_folder = "dataset/amazon/2014"
train.dataset = %RecDataset.AMAZON
train.dataset_split = "beauty"
train.force_dataset_process = False
train.use_kmeans_init = True

# PQ-VAE Architecture
train.vae_input_dim = 768  # Sentence-T5-XL embeddings
train.vae_embed_dim = 32   # Patch embedding dimension
train.vae_hidden_dims = [512, 256, 128]  # MLP hidden layers
train.vae_codebook_size = 256  # Vocabulary size per codebook
train.vae_codebook_normalize = False
train.vae_codebook_mode = %QuantizeForwardMode.ROTATION_TRICK
train.vae_sim_vq = False
train.vae_n_cat_feats = 0
train.commitment_weight = 0.25

# Product Quantization Settings
train.num_codebooks = 4  # Number of independent codebooks (semantic aspects)

# =============================================================================
# Patch Encoder Settings (THIS IS THE KEY!)
# =============================================================================
train.use_patch_encoder = True  # Enable patch-based encoding

# Patch Encoder Architecture
train.patch_num_patches = 4      # Number of semantic patches to extract
train.patch_hidden_dim = 256     # Hidden dimension for patch processing
train.patch_num_heads = 4        # Multi-head attention heads
train.patch_dropout = 0.1        # Dropout rate
train.patch_diversity_weight = 0.1  # Weight for diversity loss

# Hybrid Mode (text patches + image global)
train.patch_hybrid_mode = False  # Set to True if using images

# =============================================================================
# Automatic Patch Processing (ENABLE THIS!)
# =============================================================================
# This enables automatic extraction and caching of token embeddings
# from Sentence-T5-XL for all items in the dataset
train.use_patch_embeddings = True

# Patch Embedding Model Settings
train.patch_model_name = "sentence-transformers/sentence-t5-xl"
train.patch_max_seq_length = 77  # Maximum token sequence length

# =============================================================================
# Image Features (Optional - currently disabled)
# =============================================================================
train.use_image_features = False
train.feature_combination_mode = "sum"

# System Settings
train.split_batches = True
train.amp = True
train.mixed_precision_type = "fp16"
train.wandb_logging = True
train.debug = False

# Run Settings
train.run_prefix = "pq-patch"  # Prefix for wandb run name
train.log_dir = "logdir/pqvae"

# Pretrained Model (Optional)
# train.pretrained_pqvae_path = "path/to/checkpoint.pt"
