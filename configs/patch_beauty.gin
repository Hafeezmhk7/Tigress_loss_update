import data.processed
import modules.quantize

# ===============================================
# PRODUCT QUANTIZATION VAE - INTEGRATED VERSION
# ===============================================
# This config uses the integrated data processing approach
# where patch embeddings are handled by ItemData automatically.

# ––– BASIC TRAINING ARGUMENTS ––––
train.iterations=400000
train.learning_rate=0.0005
train.weight_decay=0.01
train.batch_size=64
train.save_model_every=80000
train.eval_every=10000
train.log_every=10000

# ––– DATASET CONFIGURATION ––––
train.dataset=%data.processed.RecDataset.AMAZON
train.force_dataset_process=False
train.dataset_folder="dataset/amazon/2014"
train.dataset_split="beauty"
train.debug=False

# ––– LOGGING ––––
train.log_dir="logdir/pqvae-patch/amazon/2014"
train.wandb_logging=False
train.run_prefix="pq"

# ––– MULTIMODAL FEATURES ––––
train.use_image_features=False
train.feature_combination_mode=""

# ––– PRODUCT QUANTIZATION PARAMETERS ––––
train.vae_input_dim=768
train.vae_hidden_dims=[512, 256, 128]
train.vae_embed_dim=32
train.vae_codebook_size=256
train.vae_codebook_normalize=False
train.vae_sim_vq=False
train.commitment_weight=0.25
train.vae_codebook_mode=%modules.quantize.QuantizeForwardMode.ROTATION_TRICK
train.vae_n_cat_feats=0

# ––– NUMBER OF CODEBOOKS ––––
train.num_codebooks=4

# ––– RESUME TRAINING ––––
train.pretrained_pqvae_path=None

# ===============================================
# INTEGRATED PATCH EMBEDDING PARAMETERS
# ===============================================

# Enable patch embeddings in ItemData
# This will automatically process and cache embeddings
train.use_patch_embeddings=True  # NEW: Enable in ItemData

# Enable patch encoder in model
train.use_patch_encoder=True

# Model name for embedding extraction
train.patch_model_name="sentence-transformers/sentence-t5-xl"

# Maximum sequence length for tokenization
train.patch_max_seq_length=77

# Number of semantic patches
train.patch_num_patches=4

# Patch encoder architecture
train.patch_hidden_dim=256
train.patch_num_heads=4
train.patch_dropout=0.1
train.patch_diversity_weight=0.1

# Hybrid mode
train.patch_hybrid_mode=False

# ===============================================
# ADVANTAGES OF INTEGRATED APPROACH
# ===============================================
# ✅ No separate precompute script needed
# ✅ Embeddings processed automatically on first run
# ✅ Cached for subsequent runs
# ✅ Seamless integration with ItemData
# ✅ Patches included in batches automatically
# ✅ Cleaner training code

# ===============================================
# USAGE
# ===============================================
# Just run:
#   python train_pqvae.py configs/pq_beauty.gin
#
# On first run:
#   - ItemData will detect missing patch embeddings
#   - Automatically extract and cache them
#   - This takes ~15 minutes
#
# On subsequent runs:
#   - Cached embeddings loaded instantly
#   - Training starts immediately
