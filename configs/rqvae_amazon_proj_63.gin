import data.processed
import modules.quantize

# ===== BASE CONFIGURATION =====
train.iterations=400000  # Reduced from 400K (based on your saturation analysis)
train.learning_rate=0.0005
train.weight_decay=0.01
train.batch_size=128
train.vae_input_dim=768
train.vae_n_layers=3
train.vae_n_cat_feats=0
train.vae_hidden_dims=[512, 256, 128]
train.vae_embed_dim=32
train.vae_codebook_size=256
train.pretrained_rqvae_path=None
train.vae_codebook_normalize=False
train.vae_sim_vq=False
train.save_model_every=10000
train.eval_every=10000
train.log_every=1000
train.dataset=%data.processed.RecDataset.AMAZON
train.commitment_weight=0.3
train.vae_codebook_mode=%modules.quantize.QuantizeForwardMode.ROTATION_TRICK
train.force_dataset_process=False
train.debug=False
train.dataset_folder="dataset/amazon/2014"
train.dataset_split="beauty"
train.log_dir="logdir/rqvae/amazon/2014"
train.wandb_logging=True
train.use_image_features=True
train.feature_combination_mode="concat"
train.use_cross_attn=False
train.attn_heads=8

# ===== PROJECTION HEAD CONFIGURATION (NEW!) =====
# This solves the magnitude collapse problem!

train.use_projection_head=True  # ✨ ENABLE PROJECTION HEAD
train.projection_dim=128  # Projection space dimension
train.projection_hidden_dim=64  # Hidden layer in projection MLP
train.projection_use_bn=True  # Batch normalization in projection
train.projection_shared=False  # Separate projection per level (recommended)

# ===== FOUNDATION-FIRST STRATEGY =====
# Your discovery: Optimize L0 with InfoNCE, let cascade improve L1/L2

train.use_encoder_infonce=False
train.use_multiscale_infonce=True
train.use_cost_infonce=False
train.infonce_temperature=0.07
train.multiscale_infonce_weight=1
train.multiscale_level_weights=[0.1, 0.0, 0.0]  # Foundation-first!
train.encoder_dropout_rate=0.1

# ===== RECONSTRUCTION LOSS (RESTORED!) =====
# Critical for preserving magnitude!

train.use_reconstruction_loss=True
train.reconstruction_weight=1.5  # Significant weight!

# ===== RUN CONFIGURATION =====

train.run_prefix="ProjHead-Foundation-[0.3-0-0]-Recon2"

# ===== EXPECTED RESULTS =====
#
# With Projection Head + Reconstruction:
#
# Embedding Norms (RESTORED!):
#   emb_avg_norm_0: ~1.0-1.5
#   emb_avg_norm_1: ~5.0-10.0  ← FIXED! (was ~0.5-1.0)
#   emb_avg_norm_2: ~1.0-1.5
#
# RQ-VAE Training:
#   ✅ Similar loss to baseline
#   ✅ 100% codebook usage at all levels
#   ✅ Embedding norms preserved
#
# Decoder Performance:
#   ✅ NDCG@5: +5-10% over baseline
#   ✅ Information capacity restored
#
# Training Time:
#   ~9 minutes (100K iterations)
#
# ===== END EXPECTED RESULTS =====

# ===== ABLATION STUDIES (UNCOMMENT TO RUN) =====

# Ablation 1: Without Projection Head (reproduces magnitude collapse)
# train.use_projection_head=False
# train.run_prefix="No-ProjHead-Foundation-[0.3-0-0]-Recon0.5"
# Expected: emb_avg_norm_1 ~0.5-1.0 (collapsed)

# Ablation 2: Projection Head + No Reconstruction
# train.use_projection_head=True
# train.use_reconstruction_loss=False
# train.run_prefix="ProjHead-Foundation-[0.3-0-0]-NoRecon"
# Expected: Norms slightly lower, but better than no projection

# Ablation 3: Different Reconstruction Weights
# train.use_projection_head=True
# train.reconstruction_weight=0.3  # Lower weight
# train.run_prefix="ProjHead-Foundation-[0.3-0-0]-Recon0.3"

# Ablation 4: Shared vs Separate Projections
# train.use_projection_head=True
# train.projection_shared=True
# train.run_prefix="ProjHead-Shared-Foundation-[0.3-0-0]-Recon0.5"

# ===== COMPARISON TO YOUR PREVIOUS EXPERIMENTS =====

# Your Previous Best (Foundation-first, no projection):
# - multiscale_level_weights=[0.3, 0.0, 0.0]
# - use_projection_head=False
# - use_reconstruction_loss=False or low weight
# Result: emb_avg_norm_1 ~0.5-1.0 (collapsed)
#         Decoder NDCG: Worse than baseline

# New Approach (Foundation-first WITH projection):
# - multiscale_level_weights=[0.3, 0.0, 0.0]  # Same!
# - use_projection_head=True  # NEW!
# - use_reconstruction_loss=True
# - reconstruction_weight=0.5  # RESTORED!
# Expected: emb_avg_norm_1 ~5.0-10.0 (restored!)
#           Decoder NDCG: +5-10% over baseline

# ===== KEY INSIGHTS =====
#
# 1. Foundation-First [0.3, 0, 0] is CORRECT strategy
#    ✅ 100% codebook usage
#    ✅ Natural hierarchy
#    ✅ Residual cascade effect
#
# 2. Problem was magnitude collapse from InfoNCE
#    ❌ InfoNCE normalizes embeddings
#    ❌ Low magnitude → low capacity
#    ❌ Decoder can't learn well
#
# 3. Solution: Projection Head
#    ✅ Separate angle (InfoNCE) from magnitude (reconstruction)
#    ✅ Original embeddings preserve magnitude
#    ✅ Projections normalized for InfoNCE
#    ✅ Best of both worlds!
#
# ===== END INSIGHTS =====
